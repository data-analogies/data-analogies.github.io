<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Data Analogies Enable Efficient Cross-Embodiment Learning">
  <meta name="keywords" content="Robot Learning, Cross-Embodiment, Data Analogies">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Data Analogies Enable Efficient Cross-Embodiment Learning</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="stylesheet" href="style.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Data Analogies Enable Efficient Cross-Embodiment Learning</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">Jonathan Yang,</span>
            <span class="author-block">Chelsea Finn,</span>
            <span class="author-block">Dorsa Sadigh</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block">Stanford University</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link -->
              <span class="link-block">
                <a href="#paper"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Video Link -->
              <span class="link-block">
                <a href="#results"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Results</span>
                </a>
              </span>
              <!-- Code Link -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming Soon)</span>
                </a>
              </span>
              <!-- Data Link -->
              <span class="link-block">
                <a href="https://huggingface.co/datasets/jonathanyang0127/xembod_robocasa"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-database"></i>
                  </span>
                  <span>Data</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="images/teaser_v3.png"
           class="teaser-image"
           alt="Data Analogies Enable Efficient Cross-Embodiment Learning"/>
      <h2 class="subtitle has-text-centered">
        How should we collect data so that demonstrations from one robot directly help another?
        We study <b>coverage</b> and <b>pairing</b> strategies across embodiments and show that
        structured, trajectory-paired datasets unlock substantial transfer for generalist robot policies.
      </h2>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Overview</h2>
        <div class="content has-text-justified">
          <p>
            Generalist robot policies are now trained on increasingly large, heterogeneous
            cross-embodiment datasets. While scale clearly helps, it remains unclear
            <em>what</em> is actually being transferred when we mix data from many robots,
            morphologies, and viewpoints.
          </p>
          <p>
            This work asks: <strong>what kinds of cross-embodiment data actually help a
            policy adapt to a new robot under a fixed budget of target demonstrations?</strong>
		  </p>
		  <p>
            We introduce the notion of <strong>data analogies</strong>, structured correspondences
            that let demonstrations from one embodiment efficiently help another, and systematically
            study how data collection along three axes (viewpoint, morphology, appearance) affects this capability.
          </p>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Data-Centric Method</h2>
        <div class="content has-text-justified">
          <p>
            We view data for cross-embodiment learning through two orthogonal design axes:
            <strong>coverage</strong> and <strong>pairing</strong>.
          </p>

          <h3 class="title is-4">Coverage</h3>
          <ul>
            <li><strong>Targeted</strong>: Select demonstrations that explicitly fill gaps relative to
                the target robot (e.g., missing camera poses, gripper types, or kinematic regimes).</li>
            <li><strong>Diverse</strong>: Collect broadly varied demonstrations without target-aware
                selection, emphasizing visual and embodiment breadth.</li>
          </ul>

          <h3 class="title is-4">Pairing</h3>
          <ul>
            <li><strong>Unpaired</strong>: Independent demonstrations linked only by task labels.</li>
            <li><strong>Task-Paired</strong>: Same task instance across robots (same objects and goals),
                weakly aligned.</li>
            <li><strong>Trajectory-Paired</strong>: Time-aligned executions with similar object-centric
                trajectories, via DTW in both simulation and the real world.</li>
          </ul>
        </div>
        <div class="has-text-centered">
          <img src="images/coverage_vs_pairing.png" alt="Coverage vs Pairing strategies" style="max-width: 100%; border-radius: 10px;"/>
          <p class="is-size-6 has-text-grey">
            <strong>Coverage vs. Pairing.</strong> Simulation scenes illustrating how diversity and
            cross-robot alignment shape transfer.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Experiments</h2>
      <div class="content has-text-justified">
        <h3 class="title is-4">Core Questions</h3>
        <ol>
          <li>Under a fixed budget, which data collection strategy works best for each axis?</li>
          <li>How does our compositional dataset compare to naive training on large open-source data?</li>
          <li>How does performance scale as we increase the diversity of source data?</li>
          <li>Do these trends hold on real robots?</li>
        </ol>
        <p>
          We evaluate Pi0.5-style VLA policies on RoboCasa-based simulation benchmarks and
          tabletop tasks on Franka, WidowX, PiperX, and related platforms, under strict
          few-shot constraints on the target robot, to study efficient cross-embodiment learning.
        </p>
      </div>
    </div>
  </div>
</section>

<section class="section" id="results">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Coverage vs. Pairing</h3>
          <p>
            Diversity helps most for perceptual shifts (viewpoint and appearance), where broad
            variation regularizes the encoder and reduces overfitting to specific scenes or cameras.
            For morphology, however, diversity alone quickly saturates: <strong>targeted coverage and
            strong trajectory pairing are essential</strong> to bridge action-space differences.
          </p>
        </div>
        <div class="has-text-centered" style="margin-top: 1.5rem;">
          <img src="images/figure1_main_coverage.png" alt="Pairing is Important" style="max-width: 100%; border-radius: 10px;"/>
          <p class="is-size-6 has-text-grey">
            <strong>Pairing is Important.</strong>
          </p>
        </div>
        <div class="content has-text-justified">
          <h3 class="title is-4">Improvements over Large, Open-Source Datasets</h3>
          <p>
            Training on large unpaired datasets like OXE provides strong baselines, but
            our method delivers large gains in both simulation and real-world evaluations.
          </p>
        </div>
        <div class="has-text-centered" style="margin-top: 1.5rem;">
          <img src="images/figure2_oxe_vs_targeted.png" alt="Comparison with OXE and targeted data" style="max-width: 100%; border-radius: 10px;"/>
          <p class="is-size-6 has-text-grey">
            <strong>Figure 2.</strong> Composed OXE+Translational data outperforms both narrow two-robot
            pools and large unpaired OXE training.
          </p>
        </div>
        <div class="has-text-centered" style="margin-top: 1.5rem;">
          <img src="images/figure3_scaling.png" alt="Scaling with diversity" style="max-width: 100%; border-radius: 10px;"/>
          <p class="is-size-6 has-text-grey">
            <strong>Figure 3.</strong> Scaling behavior as we increase the diversity of source
            embodiments, viewpoints, and scenes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3 has-text-centered">Evaluations</h2>
      <p class="has-text-centered">Example policy rollouts across different embodiments and viewpoints.</p>

      <div id="Evaluations">
        <div class="video-row">
          <div class="video-col">
            <video autoplay loop muted playsinline>
              <source src="evaluation_images/panda_episode_02_main.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-col">
            <video autoplay loop muted playsinline>
              <source src="evaluation_images/panda_episode_03_main.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-col">
            <video autoplay loop muted playsinline>
              <source src="evaluation_images/episode_000005.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-col">
            <video autoplay loop muted playsinline>
              <source src="evaluation_images/episode_000006.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Real-Robot Transfer</h2>
        <div class="content has-text-justified">
          <p>
            We validate our findings on real robot platforms including Franka, WidowX, and PiperX
            in kitchen-like scenes. Structured coverage and trajectory pairing reliably improve
            success by 25-40 points over baselines.
          </p>
        </div>
        <div class="columns is-centered" style="margin-top: 1.5rem;">
          <div class="column">
            <div class="has-text-centered">
              <img src="images/sim_robots.png" alt="Simulation robots" style="border-radius: 10px;"/>
              <p class="is-size-6 has-text-grey">
                <strong>Simulation robots.</strong> Diverse simulated embodiments and viewpoints.
              </p>
            </div>
          </div>
          <div class="column">
            <div class="has-text-centered">
              <img src="images/real_robots.png" alt="Real robots" style="border-radius: 10px;"/>
              <p class="is-size-6 has-text-grey">
                <strong>Real robots.</strong> Physical setups with Franka, WidowX, PiperX.
              </p>
            </div>
          </div>
        </div>
        <div class="has-text-centered" style="margin-top: 1.5rem;">
          <img src="images/figure4_real_world.png" alt="Real-world transfer results" style="max-width: 100%; border-radius: 10px;"/>
          <p class="is-size-6 has-text-grey">
            <strong>Figure 4.</strong> Real-world transfer results across multiple robot pairs and tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="paper">
  <div class="container is-max-desktop content">
    <h2 class="title is-3">Paper</h2>
    <p>
      This website summarizes the main ideas, methods, and results from our paper on
      <strong>Data Analogies Enable Efficient Cross-Embodiment Learning</strong>. Please refer to the full paper PDF for complete
      details, ablations, and experimental tables.
    </p>
    <div class="publication-links" style="margin: 1.5rem 0;">
      <span class="link-block">
        <a href="#"
           class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
              <i class="fas fa-file-pdf"></i>
          </span>
          <span>Paper PDF (Coming Soon)</span>
        </a>
      </span>
      <span class="link-block">
        <a href="#"
           class="external-link button is-normal is-rounded is-dark">
          <span class="icon">
              <i class="ai ai-arxiv"></i>
          </span>
          <span>arXiv (Coming Soon)</span>
        </a>
      </span>
    </div>

    <h2 class="title is-4">BibTeX</h2>
    <pre><code>@inproceedings{data_analogies_2025,
  title     = {Data Analogies Enable Efficient Cross-Embodiment Learning},
  booktitle = {CVPR},
  year      = {2025}
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website was adapted from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

<script>
function openTab(evt, tabName) {
  var i, tabcontent, tablinks;
  tabcontent = document.getElementsByClassName("tabcontent");
  for (i = 0; i < tabcontent.length; i++) {
    tabcontent[i].style.display = "none";
  }
  tablinks = document.getElementsByClassName("tablinks");
  for (i = 0; i < tablinks.length; i++) {
    tablinks[i].className = tablinks[i].className.replace(" active", "");
  }
  document.getElementById(tabName).style.display = "block";
  evt.currentTarget.className += " active";
}

// Open default tab
document.getElementById("defaultOpen").click();
</script>

</body>
</html>
